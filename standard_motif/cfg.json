{
  "algo": "APPO",
  "env": "nle_fixed_eat_action",
  "msg_count_bonus": 0.0,
  "beta_count_exponent": 3.0,
  "eps_threshold_quantile": 0.5,
  "rew_norm": true,
  "root_env": "NetHackScore-v1",
  "checkpoint_num": 0,
  "wandb": false,
  "reward_encoder": "nle_torchbeast_encoder",
  "encoder_embedding_dim": 128,
  "encoder_hidden_dim": 512,
  "encoder_final_activ": "ln",
  "encoder_crop_dim": 16,
  "encoder_num_layers": 2,
  "encoder_msg_model": "lt_cnn",
  "use_crop": false,
  "use_glyphs": false,
  "use_blstats": false,
  "experiment": "standard_motif",
  "experiments_root": null,
  "help": false,
  "train_dir": "train_dir/rl_saving_dir",
  "reward_dir": "train_dir/reward_saving_dir/standard_reward/",
  "device": "gpu",
  "seed": 777,
  "extrinsic_reward": 0.1,
  "save_every_sec": 120,
  "save_every_steps": 2000000,
  "keep_checkpoints": 5,
  "checkpoint_id": -1,
  "save_milestones_sec": -1,
  "stats_avg": 1000,
  "learning_rate": 0.0001,
  "train_for_env_steps": 400000000,
  "train_for_seconds": 10000000000,
  "obs_subtract_mean": 0.0,
  "obs_scale": 255.0,
  "pop_bl": true,
  "gamma": 0.99,
  "reward_scale": 0.1,
  "reward_clip": 10.0,
  "encoder_type": "conv",
  "encoder_subtype": "convnet_simple",
  "encoder_custom": "nle_rgbcrop_encoder",
  "encoder_extra_fc_layers": 1,
  "hidden_size": 512,
  "nonlinearity": "elu",
  "policy_initialization": "orthogonal",
  "policy_init_gain": 1.0,
  "actor_critic_share_weights": true,
  "use_spectral_norm": false,
  "adaptive_stddev": true,
  "initial_stddev": 1.0,
  "experiment_summaries_interval": 20,
  "adam_eps": 1e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "gae_lambda": 0.95,
  "rollout": 32,
  "num_workers": 8,
  "recurrence": 32,
  "use_rnn": true,
  "rnn_type": "gru",
  "rnn_num_layers": 1,
  "ppo_clip_ratio": 0.1,
  "ppo_clip_value": 1.0,
  "batch_size": 4096,
  "num_batches_per_iteration": 1,
  "ppo_epochs": 1,
  "num_minibatches_to_accumulate": -1,
  "max_grad_norm": 4.0,
  "exploration_loss_coeff": 0.003,
  "value_loss_coeff": 0.5,
  "exploration_loss": "entropy",
  "num_envs_per_worker": 8,
  "worker_num_splits": 2,
  "num_policies": 1,
  "policy_workers_per_policy": 1,
  "max_policy_lag": 10000,
  "traj_buffers_excess_ratio": 1.3,
  "decorrelate_experience_max_seconds": 10,
  "decorrelate_envs_on_one_worker": true,
  "with_vtrace": true,
  "vtrace_rho": 1.0,
  "vtrace_c": 1.0,
  "set_workers_cpu_affinity": true,
  "force_envs_single_thread": true,
  "reset_timeout_seconds": 120,
  "default_niceness": 0,
  "train_in_background_thread": true,
  "learner_main_loop_num_cores": 1,
  "actor_worker_gpus": [],
  "with_pbt": false,
  "pbt_mix_policies_in_one_env": true,
  "pbt_period_env_steps": 5000000,
  "pbt_start_mutation": 20000000,
  "pbt_replace_fraction": 0.3,
  "pbt_mutation_rate": 0.15,
  "pbt_replace_reward_gap": 0.1,
  "pbt_replace_reward_gap_absolute": 1e-06,
  "pbt_optimize_batch_size": false,
  "pbt_target_objective": "true_reward",
  "use_cpc": false,
  "cpc_forward_steps": 8,
  "cpc_time_subsample": 6,
  "cpc_forward_subsample": 2,
  "benchmark": false,
  "sampler_only": false,
  "env_frameskip": null,
  "env_framestack": 4,
  "pixel_format": "CHW",
  "save_ttyrec_every": 0,
  "save_dir": "dataset_dir/rl_agent",
  "custom_env_episode_len": 400,
  "llm_reward": 0.1,
  "command_line": "--algo APPO --env nle_fixed_eat_action --num_workers 8 --num_envs_per_worker 8 --batch_size 4096 --reward_scale 0.1 --obs_scale 255.0 --train_for_env_steps 400_000_000 --save_every_steps 2_000_000 --keep_checkpoints 5 --stats_avg 1000 --seed 777 --reward_dir train_dir/reward_saving_dir/standard_reward/ --experiment standard_motif --train_dir train_dir/rl_saving_dir --extrinsic_reward 0.1 --llm_reward 0.1 --reward_encoder nle_torchbeast_encoder --root_env NetHackScore-v1 --beta_count_exponent 3 --eps_threshold_quantile 0.5",
  "cli_args": {
    "algo": "APPO",
    "env": "nle_fixed_eat_action",
    "beta_count_exponent": 3.0,
    "eps_threshold_quantile": 0.5,
    "root_env": "NetHackScore-v1",
    "reward_encoder": "nle_torchbeast_encoder",
    "experiment": "standard_motif",
    "train_dir": "train_dir/rl_saving_dir",
    "reward_dir": "train_dir/reward_saving_dir/standard_reward/",
    "seed": 777,
    "extrinsic_reward": 0.1,
    "save_every_steps": 2000000,
    "keep_checkpoints": 5,
    "stats_avg": 1000,
    "train_for_env_steps": 400000000,
    "obs_scale": 255.0,
    "reward_scale": 0.1,
    "num_workers": 8,
    "batch_size": 4096,
    "num_envs_per_worker": 8,
    "llm_reward": 0.1
  },
  "git_hash": "unknown",
  "git_repo_name": "not a git repository"
}